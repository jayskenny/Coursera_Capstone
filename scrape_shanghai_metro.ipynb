{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim, GoogleV3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#set the proxy so the geopy library can use it to visit external web sites\n",
    "proxy_addr = '135.245.48.34:8000'\n",
    "os.environ['http_proxy'] = proxy_addr\n",
    "os.environ['https_proxy'] = proxy_addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geodata_single_metro_station(url):\n",
    "    \n",
    "    lat =''\n",
    "    log=''\n",
    "    station_page = requests.get(url, timeout=20)\n",
    "    soup = BeautifulSoup(station_page.text, 'html.parser')\n",
    "    #head_text= soup.head\n",
    "    #print (\"head_text =\", head_text)\n",
    "    geodata = soup.select('span.geo')\n",
    "    #print ('geodata[0] = {}'.format(geodata[0]))\n",
    "    if len(geodata)>0:\n",
    "        item = geodata[0].get_text(strip=True)\n",
    "        lat = item.split(';')[0].strip()\n",
    "        log = item.split(';')[1].strip()\n",
    "    print ('latitude: {}\\nlogitude: {}'.format(lat, log))\n",
    "    return lat, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude: 31.186422\n",
      "logitude: 121.427615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('31.186422', '121.427615')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_geodata_single_metro_station('https://en.wikipedia.org/wiki/Yishan_Road_station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shanghai_metro_stations_information(read_from_csv=False):\n",
    "    \"\"\"\n",
    "    It will first fetch the borugh and neighborhood table from wikipedia. Then go to each     link of the neighborhoods in the table. Finally, it will create a dataframe cotaining     Borough, Neighborhood and Population. It has a functionality to store the data in csv     format, and it is possible to read this data from csv to reduce time consuming            operations later.\n",
    "    \"\"\"\n",
    "    csv = open('metro_shanghai.csv', 'a')\n",
    "    #csv.write(\";Shanghai metro station information: District,Station,Latitude,Longitude\\n\")\n",
    "    print (\"haha\")\n",
    "    if not read_from_csv:\n",
    "        WIKI_LINK = \"https://en.wikipedia.org/wiki/List_of_Shanghai_Metro_stations\"\n",
    "        ROOT_WIKI_LINK = \"https://en.wikipedia.org\"\n",
    "        page = requests.get(WIKI_LINK, timeout = 10)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        station_list = []\n",
    "        soup_items = len(soup.select(\"table.wikitable tr\"))\n",
    "        print ('Number of items in the soup of the  pages = {}'.format(soup_items))\n",
    "        test_row = 0\n",
    "        count = 0\n",
    "        district =''\n",
    "        for table_row in soup.select(\"table.wikitable tr\"):\n",
    "            cells = table_row.findAll('td')\n",
    "            #print (\"table_row = soup.select(table.wikitable tr) = {}\".format(table_row))\n",
    "            #print (\"cells = table_row.findAll(td) = {}\".format(cells))\n",
    "            print (\"length of cells = {}\\n\\n\".format(len(cells)))\n",
    "            #test_row +=1\n",
    "            #if test_row >15:\n",
    "            #    print (\"station_list contains {}\".format(station_list))\n",
    "            #    return\n",
    "            if len(cells) > 5:\n",
    "                station_found = False\n",
    "                try:\n",
    "                    for i in range(8):\n",
    "                        print (\"cells[i] = {}\".format(cells[i]))\n",
    "                        text = cells[i].text.strip().replace('\\xa0',' ')\n",
    "                        print (\"text = cell[i].text.strip() ={}\\n\".format(text))\n",
    "                        if len(text) > 3 and not station_found: \n",
    "                            if not ('/' in text): \n",
    "                                station = cells[i].text.strip().replace('\\xa0', ' ')\n",
    "                                station_found = True\n",
    "                                station_link = cells[i].findAll('a')[0]['href']\n",
    "                                lat, log = get_geodata_single_metro_station(ROOT_WIKI_LINK + station_link)\n",
    "                                continue\n",
    "                        if station_found:\n",
    "                            link=cells[i].findAll('a')\n",
    "                            if len(link)>0:\n",
    "                                if ('District' in link[0]['href']):\n",
    "                                    district = text.split('/')[0]\n",
    "                                    print (\"new district found = {}\".format(district))\n",
    "                                    break\n",
    "                except:\n",
    "                    pass\n",
    "                if station_found:\n",
    "                    count += 1\n",
    "                    print (\"Add station {} = {} - {}\".format(count, district, station))\n",
    "                    station_list.append([district, station, lat, log])\n",
    "                    csv.write(\"{},{},{},{},{}\\n\".format(count,district,station,lat,log))\n",
    "                    csv.flush()\n",
    "                    time.sleep(30)\n",
    "\n",
    "        #stations = list(dict.fromkeys(station_list))\n",
    "        print (\"station_list contains {}\".format(station_list))\n",
    "        print (\"There are total {} stations in Shanghai Metro\".format(len(station_list)))\n",
    "        csv.close()\n",
    "        return\n",
    "        df = pd.DataFrame(station_list, columns=[\n",
    "                          \"Borough\", \"Neighborhood\", \"Population\"])\n",
    "        df.to_csv('population.csv')\n",
    "    else:\n",
    "        df = pd.read_csv('population.csv')\n",
    "    df = df.sort_values(by=['Borough'])\n",
    "    df = df.drop_duplicates(subset='Neighborhood', keep='last')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shanghai_metro_stations_information()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
